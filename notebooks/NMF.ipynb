{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "header = ['Tweet ID', 'timestamp', 'week', 'user_id', 'state', 'original text', 'with_emoji_text', 'without_emoji_text','in_reply_to_status_id_str',  'emoji',\n",
    "          'hashtag', 'media(type, url)', 'user_mentions', 'language']\n",
    "\n",
    "data_2015_06 = pd.read_csv('output_2015-06.csv', header=None, names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>week</th>\n",
       "      <th>user_id</th>\n",
       "      <th>state</th>\n",
       "      <th>original text</th>\n",
       "      <th>with_emoji_text</th>\n",
       "      <th>without_emoji_text</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>emoji</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>media(type, url)</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611053325644005376</td>\n",
       "      <td>1.434536e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>35298429</td>\n",
       "      <td>16</td>\n",
       "      <td>She says so unenthused</td>\n",
       "      <td>She says so unenthused</td>\n",
       "      <td>She says so unenthused</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611162865551192064</td>\n",
       "      <td>1.434562e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2537204392</td>\n",
       "      <td>16</td>\n",
       "      <td>\"For every ailment under the sun there is a re...</td>\n",
       "      <td>\"For every ailment under the sun there is a re...</td>\n",
       "      <td>\"For every ailment under the sun there is a re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>611058983927836673</td>\n",
       "      <td>1.434537e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>952640504</td>\n",
       "      <td>16</td>\n",
       "      <td>@jameshull88 yeah but still rest of conditioni...</td>\n",
       "      <td>yeah but still rest of conditioning this summer 😕</td>\n",
       "      <td>yeah but still rest of conditioning this summer</td>\n",
       "      <td>6.110588e+17</td>\n",
       "      <td>😕</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>444172192</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611165766952591360</td>\n",
       "      <td>1.434563e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2216437134</td>\n",
       "      <td>16</td>\n",
       "      <td>Reminded of how blessed I am when walking into...</td>\n",
       "      <td>Reminded of how blessed I am when walking into...</td>\n",
       "      <td>Reminded of how blessed I am when walking into...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['studio7x47', 'HeritageBuilding']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611159320655347712</td>\n",
       "      <td>1.434561e+09</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>14752306</td>\n",
       "      <td>16</td>\n",
       "      <td>@thereval; \\n\"Unarmed White Teen Shot by Black...</td>\n",
       "      <td>; \"Unarmed White Teen Shot by Black Officer\"</td>\n",
       "      <td>; \"Unarmed White Teen Shot by Black Officer\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['WheresTheOutrage', 'HandsUpDontShoot']</td>\n",
       "      <td>('photo', 'https://pbs.twimg.com/media/CHtGIEU...</td>\n",
       "      <td>42389136</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tweet ID     timestamp       week     user_id  state  \\\n",
       "0  611053325644005376  1.434536e+09  Wednesday    35298429     16   \n",
       "1  611162865551192064  1.434562e+09  Wednesday  2537204392     16   \n",
       "2  611058983927836673  1.434537e+09  Wednesday   952640504     16   \n",
       "3  611165766952591360  1.434563e+09  Wednesday  2216437134     16   \n",
       "4  611159320655347712  1.434561e+09  Wednesday    14752306     16   \n",
       "\n",
       "                                       original text  \\\n",
       "0                             She says so unenthused   \n",
       "1  \"For every ailment under the sun there is a re...   \n",
       "2  @jameshull88 yeah but still rest of conditioni...   \n",
       "3  Reminded of how blessed I am when walking into...   \n",
       "4  @thereval; \\n\"Unarmed White Teen Shot by Black...   \n",
       "\n",
       "                                     with_emoji_text  \\\n",
       "0                             She says so unenthused   \n",
       "1  \"For every ailment under the sun there is a re...   \n",
       "2  yeah but still rest of conditioning this summer 😕   \n",
       "3  Reminded of how blessed I am when walking into...   \n",
       "4       ; \"Unarmed White Teen Shot by Black Officer\"   \n",
       "\n",
       "                                  without_emoji_text  \\\n",
       "0                             She says so unenthused   \n",
       "1  \"For every ailment under the sun there is a re...   \n",
       "2    yeah but still rest of conditioning this summer   \n",
       "3  Reminded of how blessed I am when walking into...   \n",
       "4       ; \"Unarmed White Teen Shot by Black Officer\"   \n",
       "\n",
       "   in_reply_to_status_id_str emoji                                   hashtag  \\\n",
       "0                        NaN   NaN                                       NaN   \n",
       "1                        NaN   NaN                                       NaN   \n",
       "2               6.110588e+17     😕                                       NaN   \n",
       "3                        NaN   NaN        ['studio7x47', 'HeritageBuilding']   \n",
       "4                        NaN   NaN  ['WheresTheOutrage', 'HandsUpDontShoot']   \n",
       "\n",
       "                                    media(type, url) user_mentions language  \n",
       "0                                                NaN           NaN       en  \n",
       "1                                                NaN           NaN       en  \n",
       "2                                                NaN     444172192       en  \n",
       "3                                                NaN           NaN       en  \n",
       "4  ('photo', 'https://pbs.twimg.com/media/CHtGIEU...      42389136       en  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015_06.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NMF model \n",
    "\n",
    "Just a test case of small dataset, will run on the whole dataset several days after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages & define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def print_topics(model, feature_names, top_k):\n",
    "    \"\"\"\n",
    "    Print the most important words of each topics\n",
    "    \"\"\"\n",
    "    for ind, topic in enumerate(model.components_):\n",
    "        print(\"Topic #\" + str(ind+1))\n",
    "        # print out top k possible features(words)\n",
    "        print([feature_names[i] for i in topic.argsort()[:-top_k-1:-1]])\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k = 20 # number of features(words) we want to print\n",
    "n_topics = 20 # number of topics\n",
    "random_seed = 1\n",
    "l1_ratio = 0.5 # regularization\n",
    "num_features = 100 # number of features we want to include in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=num_features)\n",
    "nmf_model = NMF(n_components=n_topics, random_state=random_seed, l1_ratio=l1_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "['the', 'weight', 'in', 'savage', 'and', 'two', 'on', 'for', 'saw', 'work', 'love', 'they', 'road', 'this', 'to', 'owyhee', 'rocks', 'outdated', 'shape', 'probably']\n",
      "Topic #2\n",
      "['id', 'rooster', 'registered', 'work', 'want', 'in', 'off', 'boise', 'to', 'weight', 'play', 'realizing', 'passing', 'that', 'or', 'just', 'on', 'outdated', 'owyhee', 'past']\n",
      "Topic #3\n",
      "['is', 'there', 'rapist', 'remedy', 'or', 'for', 'be', 'owyhee', 'outdated', 'rocks', 'the', 'to', 'this', 'in', 'some', 'passing', 'maddie', 'me', 'porn', 'much']\n",
      "Topic #4\n",
      "['black', 'white', 'for', 'shape', 'probably', 'into', 'just', 'get', 'two', 'back', 'to', 'so', 'you', 'be', 'remedy', 'in', 'or', 'my', 'night', 'نقول']\n",
      "Topic #5\n",
      "['today', 'of', 'there', 'was', 'some', 'so', 'into', 'my', 'how', 'three', 'pressure', 'remedy', 'night', 'all', 'for', 'poodles', 'relieve', 'in', 'love', 'the']\n",
      "Topic #6\n",
      "['maddie', 'in', 'you', 'want', 'rapist', 'me', 'to', 'there', 'night', 'savage', 'just', 'get', 'and', 'even', 'work', 'don', 'be', 'on', 'season', 'rip']\n",
      "Topic #7\n",
      "['summer', 'this', 'of', 'saw', 'two', 'don', 'rocks', 'owyhee', 'outdated', 'play', 'passing', 'realizing', 'much', 'or', 'that', 'rip', 'past', 'season', 'is', 'and']\n",
      "Topic #8\n",
      "['with', 'run', 'owyhee', 'outdated', 'rocks', 'some', 'again', 'gonna', 'be', 'on', 'the', 'this', 'to', 'is', 'it', 'in', 'die', 'night', 'of', 'black']\n",
      "Topic #9\n",
      "['to', 'we', 'aka', 'and', 'quick', 'seattle', 'off', 'my', 'pressure', 'want', 'sasuke', 'relieve', 'or', 'all', 'much', 'they', 'so', 'die', 'work', 'in']\n",
      "Topic #10\n",
      "['deals', 'june', 'نقول', 'نردها', 'ما', 'لها', 'ومنكم', 'night', 'off', 'of', 'me', 'my', 'much', 'or', 'maddie', 'love', 'on', 'passing', 'outdated', 'owyhee']\n",
      "Topic #11\n",
      "['ولا', 'نقول', 'نردها', 'ما', 'لها', 'savage', 'in', 'off', 'boise', 'play', 'passing', 'realizing', 'much', 'that', 'or', 'again', 'season', 'past', 'rip', 'be']\n",
      "Topic #12\n",
      "['so', 'says', 'she', 'don', 'even', 'prolly', 'pissed', 'and', 'you', 'for', 'just', 'get', 'probably', 'shape', 'two', 'into', 'back', 'night', 'be', 'me']\n",
      "Topic #13\n",
      "['do', 'why', 'my', 'don', 'this', 'they', 'road', 'love', 'pressure', 'much', 'on', 'the', 'relieve', 'to', 'you', 'for', 'passing', 'play', 'realizing', 'be']\n",
      "Topic #14\n",
      "['got', 'back', 'rachel', 'how', 'past', 'rip', 'season', 'this', 'some', 'in', 'to', 'for', 'into', 'shape', 'probably', 'get', 'just', 'play', 'passing', 'realizing']\n",
      "Topic #15\n",
      "['it', 'porn', 'again', 'be', 'that', 'and', 'play', 'passing', 'realizing', 'three', 'much', 'night', 'or', 'all', 'to', 'run', 'how', 'poodles', 'on', 'in']\n",
      "Topic #16\n",
      "['boise', 'ومنكم', 'منا', 'من', 'ramadan', 'off', 'in', 'id', 'savage', 'that', 'outdated', 'rocks', 'owyhee', 'realizing', 'passing', 'play', 'some', 'again', 'much', 'maddie']\n",
      "Topic #17\n",
      "['me', 'gonna', 'die', 'want', 'run', 'on', 'be', 'to', 'again', 'aka', 'you', 'the', 'sasuke', 'and', 'maddie', 'get', 'just', 'even', 'don', 'it']\n",
      "Topic #18\n",
      "['when', 'into', 'how', 'my', 'of', 'today', 'three', 'all', 'night', 'poodles', 'love', 'that', 'it', 'the', 'aka', 'you', 'sasuke', 'die', 'get', 'just']\n",
      "Topic #19\n",
      "['have', 'to', 'want', 'work', 'for', 'pressure', 'be', 'much', 'again', 'relieve', 'shape', 'probably', 'they', 'run', 'so', 'into', 'remedy', 'in', 'my', 'there']\n",
      "Topic #20\n",
      "['اخاف', 'random', 'this', 'rocks', 'outdated', 'owyhee', 'some', 'don', 'the', 'is', 'لها', 'ما', 'نردها', 'نقول', 'night', 'love', 'just', 'me', 'much', 'my']\n"
     ]
    }
   ],
   "source": [
    "train = list(data_2015_06[\"without_emoji_text\"][:50])\n",
    "test = list(data_2015_06[\"without_emoji_text\"][50:70])\n",
    "X = vectorizer.fit_transform(train)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "nmf = nmf_model.fit(X)\n",
    "\n",
    "print_topics(nmf, feature_names, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model  \n",
    "output will be probabilities of each topic for every tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 1.52871611e-02, 0.00000000e+00, 3.45553134e-39,\n",
       "       1.03042963e-01, 0.00000000e+00, 4.31155215e-02, 0.00000000e+00,\n",
       "       7.14722258e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.60419488e-01, 1.17725162e-01, 4.12111400e-02,\n",
       "       0.00000000e+00, 2.37999256e-02, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test = vectorizer.fit_transform(test)\n",
    "result = nmf.transform(tfidf_test)\n",
    "print(result.shape)\n",
    "result[0] # probabilites for first test tweet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
