{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import csv,sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/scratch/ql819/Tweets/glove/glove.twitter.27B.100d.txt'\n",
    "#glove_vocab = []\n",
    "#glove_embed = []\n",
    "embedding_dict = {}\n",
    "\n",
    "file = open(file_name, 'r', encoding = 'UTF-8')\n",
    "\n",
    "for line in file.readlines():\n",
    "    row = line.strip().split(' ')\n",
    "    vocab_word = row[0]\n",
    "    #glove_vocab.append(vocab_word)\n",
    "    embed_vector = [float(i) for i in row[1:]]\n",
    "    embedding_dict[vocab_word] = np.array(embed_vector)\n",
    "    #glove_embed.append(embed_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.070878 , -0.38237  ,  0.19812  , -0.56706  , -0.10155  ,\n",
       "        0.26353  ,  0.78261  ,  0.43845  ,  0.18323  ,  0.21758  ,\n",
       "        0.14789  , -0.49593  , -2.8569   ,  0.46476  ,  1.1635   ,\n",
       "       -0.10135  , -0.80285  , -0.53834  , -0.11441  ,  0.19423  ,\n",
       "       -0.20462  ,  1.0603   ,  0.55811  , -0.083468 , -0.58797  ,\n",
       "        0.48816  ,  0.15013  , -0.65217  , -0.75158  , -0.0047128,\n",
       "       -0.039005 , -0.16217  ,  0.41973  , -0.11195  ,  0.39354  ,\n",
       "        0.14858  ,  0.95279  , -0.36303  ,  0.51631  , -0.20326  ,\n",
       "       -0.0095533, -1.1235   , -0.2453   ,  0.43147  ,  0.62233  ,\n",
       "        0.43189  , -0.28003  ,  0.16717  ,  0.55168  , -0.6711   ,\n",
       "        0.10667  , -0.39833  ,  0.092899 ,  0.12986  ,  0.16449  ,\n",
       "        0.059435 ,  0.25194  ,  0.11007  , -0.4768   , -0.35162  ,\n",
       "        0.57028  , -0.55725  , -0.45958  , -0.52199  , -0.39543  ,\n",
       "       -0.058108 ,  0.53706  ,  0.8578   ,  0.91095  , -0.54477  ,\n",
       "       -0.044107 , -0.36909  , -0.17281  ,  0.16893  , -0.33108  ,\n",
       "        1.024    ,  0.18423  , -0.22912  , -0.17935  , -0.79898  ,\n",
       "        0.96692  , -0.20633  , -0.57704  ,  0.677    ,  0.093481 ,\n",
       "        0.34034  ,  0.50956  , -0.084767 ,  0.35818  , -0.34246  ,\n",
       "       -0.098645 ,  0.062952 , -0.043667 ,  0.13669  , -0.86895  ,\n",
       "       -0.23676  ,  0.14744  ,  0.01738  , -0.12523  ,  0.50906  ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict['wrestling']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Key Word and calculate embedding representation for each key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: Mean of empty slice\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "activity_key_words = pickle.load(open('../store_key_word/activity.pickle', 'rb'))\n",
    "activity_key_words = [item.strip() for item in activity_key_words]\n",
    "\n",
    "key_words_embedding_activity = {}\n",
    "not_match = np.array([np.nan] * 100)\n",
    "for key_item in activity_key_words:\n",
    "    list_np = [embedding_dict.get(word,not_match) for word in key_item.split(' ')]\n",
    "    list_np = np.vstack(list_np)\n",
    "    key_words_embedding_activity[key_item] = np.nanmean(list_np, axis=0)\n",
    "    \n",
    "key_words_embedding_activity = dict(filter(lambda x: not np.isnan(x[1])[0], key_words_embedding_activity.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-cpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: Mean of empty slice\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "food_key_words = pickle.load(open('../store_key_word/food.pickle', 'rb'))\n",
    "food_key_words = [item.strip() for item in food_key_words]\n",
    "#food_key_words.append('hamburger')\n",
    "\n",
    "key_words_embedding_food = {}\n",
    "not_match = np.array([np.nan] * 100)\n",
    "for key_item in food_key_words:\n",
    "    list_np = [embedding_dict.get(word,not_match) for word in key_item.split(' ')]\n",
    "    list_np = np.vstack(list_np)\n",
    "    key_words_embedding_food[key_item] = np.nanmean(list_np, axis=0)\n",
    "    \n",
    "key_words_embedding_food = dict(filter(lambda x: not np.isnan(x[1])[0], key_words_embedding_food.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.7545e-02,  1.1898e-01,  2.6445e-01, -7.6977e-01,  8.6135e-02,\n",
       "        4.9159e-01,  6.5186e-01,  3.3472e-01, -2.8600e-01,  1.8043e-01,\n",
       "        4.3625e-01,  2.5313e-01, -3.7646e+00,  3.6397e-01, -6.4090e-01,\n",
       "       -4.9278e-01, -3.2588e-01, -8.0788e-02, -5.5677e-01, -2.4818e-01,\n",
       "       -2.6804e-02, -1.4919e-01, -6.3853e-02, -3.5695e-01,  2.2165e-01,\n",
       "        2.7029e-02, -1.9695e-01,  1.5099e-01, -1.1226e-01,  1.5583e-01,\n",
       "        2.1070e-01,  1.4041e-01, -2.9264e-01,  5.6477e-01,  2.0825e-03,\n",
       "        3.1600e-01,  1.8472e-01, -4.9405e-01, -3.3496e-01,  2.9211e-01,\n",
       "       -5.4029e-01,  8.7934e-01, -3.9010e-01,  6.5981e-02,  3.6901e-01,\n",
       "       -2.8464e-02,  1.5208e-01,  7.1456e-01,  4.8595e-01,  2.2912e-01,\n",
       "       -2.4792e-01, -2.3986e-01,  5.0812e-01,  5.0515e-01, -3.8752e-01,\n",
       "       -2.4902e-01,  1.7928e-02,  5.8749e-02, -3.0073e-01, -2.6155e-02,\n",
       "        5.7714e-03, -1.3286e-01,  4.3251e-04, -1.3966e-01,  3.6019e-01,\n",
       "       -4.0055e-01, -1.2270e-01, -4.3459e-02,  1.2906e-01, -2.9439e-01,\n",
       "        1.4937e-02,  3.1430e-01, -5.8159e-01, -1.7662e-01,  9.8779e-02,\n",
       "       -1.0283e-01,  4.9843e-01,  3.4365e-02,  3.1538e-01, -5.2122e-01,\n",
       "        1.7489e+00, -2.4173e-01, -7.6807e-02,  7.2640e-02,  5.3084e-01,\n",
       "       -9.7870e-02, -3.0467e-01,  4.9571e-02, -2.9675e-01, -1.9004e-01,\n",
       "       -2.7380e-01, -2.6248e-01, -8.3966e-01, -6.4233e-01,  3.8827e-01,\n",
       "       -8.2889e-02,  6.5141e-02,  1.8283e-02,  4.3190e-03,  1.1362e-01])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_words_embedding_activity['run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42112  , -0.21374  ,  0.0040391, -0.19908  ,  0.053068 ,\n",
       "       -0.11377  ,  0.31579  , -0.58126  , -0.43308  , -0.17787  ,\n",
       "        0.29681  ,  0.18923  , -1.4589   ,  0.10148  , -0.41679  ,\n",
       "        0.13242  ,  0.64401  , -0.36636  ,  0.40502  ,  0.10571  ,\n",
       "        0.16061  , -0.27589  , -1.1044   , -0.52718  ,  0.48075  ,\n",
       "        0.48371  , -0.1668   , -0.27481  , -0.19933  , -0.30383  ,\n",
       "        0.092113 ,  0.22367  , -0.21019  , -0.74207  , -0.49321  ,\n",
       "       -0.91909  ,  0.086601 ,  0.51941  ,  0.40646  ,  0.61456  ,\n",
       "       -0.0051479, -0.33896  , -0.047362 ,  0.56601  ,  0.86746  ,\n",
       "        0.28095  , -0.16091  ,  0.63973  ,  1.2355   , -0.76863  ,\n",
       "       -0.59568  , -0.15055  ,  0.36573  ,  0.37697  ,  0.33415  ,\n",
       "       -0.62394  ,  1.2061   , -0.59272  ,  0.83349  ,  0.69309  ,\n",
       "       -0.40331  , -0.23807  , -0.75458  ,  0.18594  ,  0.31011  ,\n",
       "       -0.0074961, -0.35564  , -0.49051  , -0.47419  , -0.64183  ,\n",
       "        0.34882  ,  0.1417   ,  0.14137  ,  0.26313  ,  1.1137   ,\n",
       "        0.28831  , -1.0545   ,  0.23588  ,  0.7794   , -0.097252 ,\n",
       "        1.0618   ,  0.92259  , -0.65247  ,  0.44158  , -0.39917  ,\n",
       "       -0.11913  , -0.026036 , -0.42907  , -1.2393   ,  0.63049  ,\n",
       "       -0.84051  ,  0.30231  ,  0.042474 , -0.37093  ,  0.0036088,\n",
       "        0.21802  , -0.19312  ,  0.82909  , -1.4211   ,  0.80467  ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_words_embedding_food['salmon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average word embedding for food and activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08789521, -0.12662818,  0.17837505, -0.25753277,  0.18943437,\n",
       "        0.27984903,  0.17782452, -0.05814933,  0.09088858, -0.21618454,\n",
       "        0.0552833 ,  0.05297766, -2.27854995,  0.1460505 ,  0.02979897,\n",
       "       -0.10999084, -0.10135598, -0.0428869 , -0.02375249,  0.03479561,\n",
       "       -0.23797928,  0.15196736,  0.17087834, -0.07052562,  0.06050996,\n",
       "        0.39400038, -0.11312828,  0.28280854, -0.15376193, -0.04082689,\n",
       "        0.04480472, -0.03182786,  0.20487094, -0.10702942,  0.08409915,\n",
       "        0.12387256,  0.24865125,  0.04848354,  0.22096025,  0.31769506,\n",
       "       -0.16933218,  0.08779802, -0.11178278,  0.13234026,  0.33968599,\n",
       "        0.11277588,  0.09413708,  0.33241916,  0.04865954, -0.07330837,\n",
       "        0.29735722,  0.01335191, -0.04835534, -0.01072055, -0.1965817 ,\n",
       "       -0.11439727,  0.2218295 , -0.10684127,  0.00446674,  0.18400208,\n",
       "        0.0366111 ,  0.09944915, -0.06515118, -0.213053  , -0.01532658,\n",
       "       -0.16755079,  0.04699103, -0.01201145,  0.30025454, -0.18208929,\n",
       "        0.02325197,  0.06731291,  0.07475175,  0.082667  , -0.04467773,\n",
       "        0.06555076,  0.1592782 , -0.14408468,  0.20138646, -0.21997365,\n",
       "        1.23226392,  0.08831361, -0.30185149, -0.04085342,  0.19021327,\n",
       "       -0.04199925,  0.11495906, -0.1892644 ,  0.25940917,  0.00880305,\n",
       "       -0.14567795, -0.05268133, -0.20479877, -0.20329956, -0.10825848,\n",
       "       -0.05362583,  0.1965701 ,  0.08102783, -0.22614878,  0.27707688])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_activity_word_embedding = [np_list for np_list in key_words_embedding_activity.values()]\n",
    "avg_activity_word_embedding = np.vstack(avg_activity_word_embedding)\n",
    "avg_activity_word_embedding = np.mean(avg_activity_word_embedding, axis=0)\n",
    "avg_activity_word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.18748498e-01, -2.92965645e-01,  2.52996252e-02,  1.84988628e-01,\n",
       "       -7.29455328e-02,  7.20710051e-02, -2.39598092e-02, -8.86421021e-02,\n",
       "        4.10698158e-02, -4.27165344e-01, -1.37892269e-01, -1.81628771e-02,\n",
       "       -1.62943874e+00, -1.71823236e-01, -1.01707025e-01,  2.07938805e-01,\n",
       "        3.87922445e-01,  5.94374872e-02,  8.80814799e-02,  4.15215148e-02,\n",
       "       -4.57780154e-02,  3.41082135e-02, -2.56759330e-01,  1.15337854e-01,\n",
       "        8.71791403e-02,  3.89588812e-01, -1.16866566e-01, -3.95778080e-02,\n",
       "        5.18544035e-02,  8.44636156e-03,  9.21848045e-04,  4.16343399e-01,\n",
       "       -1.20451218e-01, -3.34404485e-01, -2.94733878e-01, -3.19710636e-01,\n",
       "       -4.90131703e-02,  9.20723694e-02,  2.64664139e-01,  5.62278246e-01,\n",
       "        3.85023448e-02, -1.16345527e-01, -3.16479447e-01,  3.02896291e-01,\n",
       "        2.51448119e-01,  1.41341923e-01, -1.12162814e-01,  4.65822328e-01,\n",
       "        3.81651331e-01, -1.99104461e-01, -5.60276236e-02, -6.74228949e-02,\n",
       "       -1.29833090e-01,  1.89855145e-01, -2.35559682e-03, -2.12536578e-01,\n",
       "        5.99385304e-01, -3.00412170e-01,  1.26010138e-01,  4.10710304e-02,\n",
       "       -3.31768643e-01, -1.94259391e-01, -3.57510882e-01,  3.08227711e-01,\n",
       "        8.29687894e-02, -3.85004239e-01,  2.13705297e-02,  2.29414015e-02,\n",
       "       -5.67633722e-02, -1.66202307e-02,  1.60086823e-01, -3.65198313e-01,\n",
       "       -8.56973405e-02,  8.27009077e-02,  1.34730112e-01,  2.69162628e-01,\n",
       "       -1.99090061e-01, -2.27367335e-02,  2.66230269e-01,  4.68381874e-01,\n",
       "        9.42028575e-01,  3.28276314e-01, -5.51982806e-01, -1.31193478e-01,\n",
       "       -1.16945765e-01, -4.50393657e-02,  6.92522584e-02, -1.56647836e-01,\n",
       "       -2.14449328e-01,  4.81768895e-02, -2.41695742e-01, -5.15949621e-02,\n",
       "        1.89119889e-01, -1.57012766e-01,  9.53973380e-02,  9.88652456e-02,\n",
       "       -5.05224052e-01,  1.83627015e-01, -3.12033800e-01,  2.54317589e-01])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_food_word_embedding = [np_list for np_list in key_words_embedding_food.values()]\n",
    "avg_food_word_embedding = np.vstack(avg_food_word_embedding)\n",
    "avg_food_word_embedding = np.mean(avg_food_word_embedding, axis=0)\n",
    "avg_food_word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another way to calculate the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "several_food_list = ['vegetables', 'fruit', 'grain', 'meat', 'milk', 'food']\n",
    "several_activity_list = ['swimming', 'walking', 'jogging', 'soccer', 'football', 'basketball', 'cycling', 'activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "several_food_list_embedding = {item:embedding_dict[item] for item in several_food_list}\n",
    "several_activity_list_embedding = {item:embedding_dict[item] for item in several_activity_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(sentence):\n",
    "    \n",
    "    list_np = [embedding_dict.get(word,not_match) for word in sentence.split(' ')]\n",
    "    list_np = [item for item in list_np if not np.isnan(item)[0]]\n",
    "    if len(list_np) >= 1:\n",
    "        list_np = np.vstack(list_np)\n",
    "        \n",
    "        #1. distance between sentence average embeding and average food and actvity embedding\n",
    "        sentence_average_embedding = np.mean(list_np, axis=0)\n",
    "        distance_1_food = np.linalg.norm(sentence_average_embedding - avg_food_word_embedding)\n",
    "        distance_1_activity = np.linalg.norm(sentence_average_embedding - avg_activity_word_embedding)\n",
    "        \n",
    "        #2. distance between sentence average embedding and word 'food' and 'activity' embedding\n",
    "        distance_2_food = np.linalg.norm(sentence_average_embedding - embedding_dict['food'])\n",
    "        distance_2_activity = np.linalg.norm(sentence_average_embedding - embedding_dict['activity'])\n",
    "        \n",
    "        #3. minimum distance between sentence word and word 'food' and 'activity' embedding\n",
    "        distance_3_food = min([np.linalg.norm(list_np[row_index] - embedding_dict['food']) \\\n",
    "                                   for row_index in range(list_np.shape[0])])\n",
    "        distance_3_activity = min([np.linalg.norm(list_np[row_index] - embedding_dict['activity']) \\\n",
    "                                   for row_index in range(list_np.shape[0])])\n",
    "        \n",
    "        #4. minimun distance between sentence work and several_food_list and several_activity_list\n",
    "        distance_4_food = []\n",
    "        distance_4_activity = []\n",
    "        for word_embedding in several_food_list_embedding.values():\n",
    "            distance_4_food.extend([np.linalg.norm(list_np[row_index] - word_embedding) \\\n",
    "                                   for row_index in range(list_np.shape[0])])\n",
    "        \n",
    "        for word_embedding in several_activity_list_embedding.values():\n",
    "            distance_4_activity.extend([np.linalg.norm(list_np[row_index] - word_embedding) \\\n",
    "                                   for row_index in range(list_np.shape[0])])\n",
    "            \n",
    "        distance_4_food = min(distance_4_food)\n",
    "        distance_4_activity = min(distance_4_activity)\n",
    "        \n",
    "        return [distance_1_food,distance_1_activity,distance_2_food,distance_2_activity,\n",
    "               distance_3_food,distance_3_activity,distance_4_food,distance_4_activity]\n",
    "        \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "if [1]:\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ql819/Tweets/token_id_time_state/2015-06_key_work_id_state_time.csv /scratch/ql819/Tweets/similar_sentence/2015-06.csv\n"
     ]
    }
   ],
   "source": [
    "from_csv_list = ['/scratch/ql819/Tweets/token_id_time_state/2014-06_key_work_id_state_time.csv',\n",
    "                 '/scratch/ql819/Tweets/token_id_time_state/2014-11_key_work_id_state_time.csv',\n",
    "                 '/scratch/ql819/Tweets/token_id_time_state/2015-06_key_work_id_state_time.csv',\n",
    "                 '/scratch/ql819/Tweets/token_id_time_state/2015-11_key_work_id_state_time.csv']\n",
    "\n",
    "to_csv_list = ['/scratch/ql819/Tweets/similar_sentence/2014-06.csv',\n",
    "                 '/scratch/ql819/Tweets/similar_sentence/2014-11.csv',\n",
    "                 '/scratch/ql819/Tweets/similar_sentence/2015-06.csv',\n",
    "                 '/scratch/ql819/Tweets/similar_sentence/2015-11.csv']\n",
    "\n",
    "#index = int(sys.argv[1])\n",
    "index = 2\n",
    "\n",
    "from_csv = from_csv_list[index]\n",
    "to_csv = to_csv_list[index]\n",
    "print(from_csv,to_csv)\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "fp = open(to_csv, \"a\") \n",
    "wr = csv.writer(fp, dialect='excel')\n",
    "\n",
    "with open(from_csv) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV: \n",
    "        tweet = row[4]\n",
    "        result = calculate_distance(tweet)\n",
    "        if result:\n",
    "            row.extend(result)\n",
    "            wr.writerow(row)\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['ID', 'timestamp', 'Day_of_Week', 'State', 'Tweet', 'mentioned_food',\n",
    "         'mentioned_activity', 'target', 'distance_1_food','distance_1_activity',\n",
    "          'distance_2_food','distance_2_activity','distance_3_food','distance_3_activity',\n",
    "          'distance_4_food','distance_4_activity']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
