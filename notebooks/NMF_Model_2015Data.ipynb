{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, the data used is the tokenized version tweets from 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets1 = pd.read_csv(\"output_2015_06.csv\",index_col=False)\n",
    "tweets1.drop('Unnamed: 0',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_15_06 = pd.read_csv(\"small_2015-06.csv\", header=None, names=['Tweet ID', 'Tokenized_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tokenized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611053325644005376</td>\n",
       "      <td>she say unenthused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611162865551192064</td>\n",
       "      <td>for ailment sun remedy none if try find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>611058983927836673</td>\n",
       "      <td>yeah rest conditioning summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611165766952591360</td>\n",
       "      <td>remind blessed i walk office today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611159320655347712</td>\n",
       "      <td>unarmed white teen shot black officer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tweet ID                         Tokenized_tweets\n",
       "0  611053325644005376                       she say unenthused\n",
       "1  611162865551192064  for ailment sun remedy none if try find\n",
       "2  611058983927836673            yeah rest conditioning summer\n",
       "3  611165766952591360       remind blessed i walk office today\n",
       "4  611159320655347712    unarmed white teen shot black officer"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_15_06.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6615177"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_15_06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_15_11 = pd.read_csv(\"small_2015-11.csv\", header=None, names=['Tweet ID', 'Tokenized_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tokenized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>667224578763067392</td>\n",
       "      <td>christmas tree s like s mansion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>667221463523581952</td>\n",
       "      <td>kenneth whalum special guests zoocru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>667224289947590656</td>\n",
       "      <td>us 129 tail of the dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>667233408611975168</td>\n",
       "      <td>i wish i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>667228363631775744</td>\n",
       "      <td>i be amp sushi charlotte nc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tweet ID                      Tokenized_tweets\n",
       "0  667224578763067392       christmas tree s like s mansion\n",
       "1  667221463523581952  kenneth whalum special guests zoocru\n",
       "2  667224289947590656             us 129 tail of the dragon\n",
       "3  667233408611975168                              i wish i\n",
       "4  667228363631775744           i be amp sushi charlotte nc"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_15_11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9255633"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_15_11) + len(token_15_06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THis time we put all 2015 data to our NMF model. There are 9,255,633 tweets in total.\n",
    "token_15 =  pd.concat([token_15_06, token_15_11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9255633"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tokenized_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611053325644005376</td>\n",
       "      <td>she say unenthused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611162865551192064</td>\n",
       "      <td>for ailment sun remedy none if try find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>611058983927836673</td>\n",
       "      <td>yeah rest conditioning summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611165766952591360</td>\n",
       "      <td>remind blessed i walk office today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611159320655347712</td>\n",
       "      <td>unarmed white teen shot black officer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tweet ID                         Tokenized_tweets\n",
       "0  611053325644005376                       she say unenthused\n",
       "1  611162865551192064  for ailment sun remedy none if try find\n",
       "2  611058983927836673            yeah rest conditioning summer\n",
       "3  611165766952591360       remind blessed i walk office today\n",
       "4  611159320655347712    unarmed white teen shot black officer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop all null value \n",
    "token_15_clean = token_15[token_15['Tokenized_tweets'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9236748"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_15_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = [len(token_15_clean.Tokenized_tweets.iloc[i]) for i in range(len(token_15_clean))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "length.sort()\n",
    "#length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from plotly import tools\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# Get the distribution of tokenized tweets length \n",
    "data = [go.Histogram(x=length)]\n",
    "layout=go.Layout(title=\"Distribution of Tokenized Tweets Length\", xaxis={'title':'Length of Tokenized Sentence'}, yaxis={'title':'Count'})\n",
    "figure=go.Figure(data=data,layout=layout)\n",
    "iplot(figure, filename='basic histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed data to NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def print_topics(model, feature_names, top_k):\n",
    "    \"\"\"\n",
    "    Print the most important words of each topics\n",
    "    \"\"\"\n",
    "    for ind, topic in enumerate(model.components_):\n",
    "        print(\"Topic #\" + str(ind+1))\n",
    "        # print out top k possible features(words)\n",
    "        print([feature_names[i] for i in topic.argsort()[:-top_k-1:-1]])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k = 20 # number of features(words) we want to print\n",
    "n_topics = 20 # number of topics\n",
    "random_seed = 1\n",
    "l1_ratio = 0.5 # regularization\n",
    "num_features = 10000 # number of features we want to include in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=num_features)\n",
    "nmf_model = NMF(n_components=n_topics, random_state=random_seed, l1_ratio=l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_tokenized = token_15_clean['Tokenized_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9236748"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are around 9 million tweets and this time put the first  5 million to feed model\n",
    "len(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 68.836s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=num_features,\n",
    "                                    stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(text_tokenized[:5000000])\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '000in',\n",
       " '001',\n",
       " '002',\n",
       " '003',\n",
       " '004',\n",
       " '005',\n",
       " '006',\n",
       " '007',\n",
       " '008',\n",
       " '009',\n",
       " '00kts',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '0700',\n",
       " '070415',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '100th',\n",
       " '101',\n",
       " '1010',\n",
       " '1015',\n",
       " '102',\n",
       " '102nd',\n",
       " '103',\n",
       " '1030',\n",
       " '104',\n",
       " '1045',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '1098',\n",
       " '10k',\n",
       " '10mi',\n",
       " '10pm',\n",
       " '10th',\n",
       " '10x',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '111',\n",
       " '1111',\n",
       " '1115',\n",
       " '112',\n",
       " '113',\n",
       " '1130',\n",
       " '114',\n",
       " '1145',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '11pm',\n",
       " '11th',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000',\n",
       " '121',\n",
       " '122',\n",
       " '122nd',\n",
       " '123',\n",
       " '1230',\n",
       " '125',\n",
       " '128',\n",
       " '129',\n",
       " '12th',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '131',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '138',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '145',\n",
       " '14th',\n",
       " '14u',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '151',\n",
       " '152',\n",
       " '155',\n",
       " '159',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '162nd',\n",
       " '165',\n",
       " '16th',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '175',\n",
       " '17802',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '181st',\n",
       " '18th',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '192',\n",
       " '1940',\n",
       " '1950',\n",
       " '1975',\n",
       " '1989',\n",
       " '1990',\n",
       " '1995',\n",
       " '1999',\n",
       " '19th',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2024',\n",
       " '205',\n",
       " '209',\n",
       " '20th',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '212',\n",
       " '215',\n",
       " '216',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '2200',\n",
       " '224',\n",
       " '225',\n",
       " '22nd',\n",
       " '23',\n",
       " '230',\n",
       " '2300',\n",
       " '231',\n",
       " '235',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '2400',\n",
       " '245',\n",
       " '247',\n",
       " '248',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '253',\n",
       " '25th',\n",
       " '26',\n",
       " '2600',\n",
       " '26th',\n",
       " '27',\n",
       " '2700',\n",
       " '275',\n",
       " '27th',\n",
       " '28',\n",
       " '280',\n",
       " '2800',\n",
       " '288',\n",
       " '28th',\n",
       " '29',\n",
       " '290',\n",
       " '2900',\n",
       " '2972inhg',\n",
       " '2979',\n",
       " '298',\n",
       " '2981',\n",
       " '2982',\n",
       " '2983',\n",
       " '2984',\n",
       " '2985',\n",
       " '2986',\n",
       " '2987',\n",
       " '2988',\n",
       " '2989',\n",
       " '299',\n",
       " '2991',\n",
       " '2992',\n",
       " '2993',\n",
       " '2994',\n",
       " '2995',\n",
       " '2996',\n",
       " '2997',\n",
       " '2998',\n",
       " '2999',\n",
       " '29th',\n",
       " '2day',\n",
       " '2k',\n",
       " '2k15',\n",
       " '2mi',\n",
       " '2nd',\n",
       " '2night',\n",
       " '2nite',\n",
       " '2x',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30000',\n",
       " '3001',\n",
       " '3002',\n",
       " '3003',\n",
       " '3004',\n",
       " '3005',\n",
       " '3006',\n",
       " '3007',\n",
       " '3008',\n",
       " '3009',\n",
       " '301',\n",
       " '3011',\n",
       " '3012',\n",
       " '3013',\n",
       " '3015',\n",
       " '3018',\n",
       " '303',\n",
       " '305',\n",
       " '30a',\n",
       " '30th',\n",
       " '31',\n",
       " '310',\n",
       " '3100',\n",
       " '311',\n",
       " '312',\n",
       " '313',\n",
       " '314',\n",
       " '315',\n",
       " '31st',\n",
       " '32',\n",
       " '320',\n",
       " '3200',\n",
       " '32801',\n",
       " '32803',\n",
       " '32804',\n",
       " '32805',\n",
       " '32807',\n",
       " '32808',\n",
       " '32811',\n",
       " '32819',\n",
       " '32822',\n",
       " '32827',\n",
       " '32835',\n",
       " '32839',\n",
       " '32nd',\n",
       " '33',\n",
       " '330',\n",
       " '3300',\n",
       " '33rd',\n",
       " '34',\n",
       " '345',\n",
       " '347',\n",
       " '34th',\n",
       " '35',\n",
       " '350',\n",
       " '3500',\n",
       " '35th',\n",
       " '36',\n",
       " '360',\n",
       " '3600',\n",
       " '365',\n",
       " '36th',\n",
       " '37',\n",
       " '3700',\n",
       " '375',\n",
       " '37th',\n",
       " '38',\n",
       " '3800',\n",
       " '38th',\n",
       " '39',\n",
       " '3900',\n",
       " '3d',\n",
       " '3mi',\n",
       " '3rd',\n",
       " '3x',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '401',\n",
       " '405',\n",
       " '40th',\n",
       " '41',\n",
       " '410',\n",
       " '4100',\n",
       " '415',\n",
       " '41st',\n",
       " '42',\n",
       " '420',\n",
       " '4200',\n",
       " '42nd',\n",
       " '43',\n",
       " '430',\n",
       " '4300',\n",
       " '43rd',\n",
       " '44',\n",
       " '4400',\n",
       " '445',\n",
       " '45',\n",
       " '450',\n",
       " '4500',\n",
       " '45th',\n",
       " '46',\n",
       " '4600',\n",
       " '47',\n",
       " '4700',\n",
       " '48',\n",
       " '4800',\n",
       " '48th',\n",
       " '49',\n",
       " '4mi',\n",
       " '4pm',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '501',\n",
       " '502',\n",
       " '50th',\n",
       " '51',\n",
       " '510',\n",
       " '5100',\n",
       " '512',\n",
       " '515',\n",
       " '52',\n",
       " '520',\n",
       " '5200',\n",
       " '53',\n",
       " '530',\n",
       " '530pm',\n",
       " '54',\n",
       " '545',\n",
       " '55',\n",
       " '550',\n",
       " '56',\n",
       " '561',\n",
       " '57',\n",
       " '574',\n",
       " '57th',\n",
       " '58',\n",
       " '5800',\n",
       " '586',\n",
       " '59',\n",
       " '595',\n",
       " '59th',\n",
       " '5k',\n",
       " '5mi',\n",
       " '5pm',\n",
       " '5sos',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '601',\n",
       " '604',\n",
       " '605',\n",
       " '606',\n",
       " '60th',\n",
       " '61',\n",
       " '610',\n",
       " '611',\n",
       " '612',\n",
       " '61215',\n",
       " '613',\n",
       " '614',\n",
       " '615',\n",
       " '616',\n",
       " '617',\n",
       " '618',\n",
       " '619',\n",
       " '62',\n",
       " '620',\n",
       " '621',\n",
       " '622',\n",
       " '623',\n",
       " '624',\n",
       " '625',\n",
       " '626',\n",
       " '62615',\n",
       " '627',\n",
       " '62715',\n",
       " '628',\n",
       " '629',\n",
       " '63',\n",
       " '630',\n",
       " '630pm',\n",
       " '632',\n",
       " '635',\n",
       " '64',\n",
       " '645',\n",
       " '646',\n",
       " '65',\n",
       " '6515',\n",
       " '66',\n",
       " '6615',\n",
       " '666',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '698',\n",
       " '6mi',\n",
       " '6pm',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '7000',\n",
       " '701',\n",
       " '702',\n",
       " '703',\n",
       " '707',\n",
       " '709',\n",
       " '71',\n",
       " '710',\n",
       " '711',\n",
       " '712',\n",
       " '713',\n",
       " '714',\n",
       " '715',\n",
       " '716',\n",
       " '717',\n",
       " '718',\n",
       " '72',\n",
       " '720',\n",
       " '725',\n",
       " '727',\n",
       " '72nd',\n",
       " '73',\n",
       " '730',\n",
       " '730pm',\n",
       " '732',\n",
       " '734',\n",
       " '739',\n",
       " '74',\n",
       " '745',\n",
       " '747',\n",
       " '75',\n",
       " '750',\n",
       " '754',\n",
       " '757',\n",
       " '76',\n",
       " '77',\n",
       " '770',\n",
       " '772',\n",
       " '78',\n",
       " '79',\n",
       " '790',\n",
       " '795',\n",
       " '7991',\n",
       " '7mi',\n",
       " '7pm',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '8000',\n",
       " '801',\n",
       " '805',\n",
       " '808',\n",
       " '81',\n",
       " '815',\n",
       " '82',\n",
       " '820',\n",
       " '827pm',\n",
       " '829pm',\n",
       " '82nd',\n",
       " '83',\n",
       " '830',\n",
       " '84',\n",
       " '845',\n",
       " '84th',\n",
       " '85',\n",
       " '86',\n",
       " '86th',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '8mi',\n",
       " '8pm',\n",
       " '8th',\n",
       " '90',\n",
       " '900',\n",
       " '9000',\n",
       " '901',\n",
       " '91',\n",
       " '910',\n",
       " '911',\n",
       " '915',\n",
       " '92',\n",
       " '92nd',\n",
       " '93',\n",
       " '930',\n",
       " '94',\n",
       " '945',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '999',\n",
       " '9mi',\n",
       " '9pm',\n",
       " '9th',\n",
       " '_ãƒ„_',\n",
       " 'a1',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aampm',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abate',\n",
       " 'abatement',\n",
       " 'abbey',\n",
       " 'abbott',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abe',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abortion',\n",
       " 'abraham',\n",
       " 'abs',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'abuse',\n",
       " 'abv',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'access',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accomplish',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accurate',\n",
       " 'accuse',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'acme',\n",
       " 'acoustic',\n",
       " 'acquire',\n",
       " 'acre',\n",
       " 'acres',\n",
       " 'acrylic',\n",
       " 'act',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activist',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'add',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adidas',\n",
       " 'adjust',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'admire',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'adopt',\n",
       " 'adoption',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adrian',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advisor',\n",
       " 'advisory',\n",
       " 'advocate',\n",
       " 'aerial',\n",
       " 'aesthetic',\n",
       " 'af',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'ag',\n",
       " 'agave',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'aggressive',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreement',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahaha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhh',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aight',\n",
       " 'aim',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airways',\n",
       " 'aisle',\n",
       " 'aj',\n",
       " 'aka',\n",
       " 'akron',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'aladdin',\n",
       " 'alameda',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'albany',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'album',\n",
       " 'albuquerque',\n",
       " 'alcatraz',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alder',\n",
       " 'aldridge',\n",
       " 'ale',\n",
       " 'alehouse',\n",
       " 'alert',\n",
       " 'ales',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alexandria',\n",
       " 'alexis',\n",
       " 'alfred',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'alki',\n",
       " 'allege',\n",
       " 'allegedly',\n",
       " 'allegheny',\n",
       " 'allen',\n",
       " 'allentown',\n",
       " 'allergic',\n",
       " 'allergy',\n",
       " 'alley',\n",
       " 'alliance',\n",
       " 'allie',\n",
       " 'alligator',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allston',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almcom',\n",
       " 'almond',\n",
       " 'aloha',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'alpharetta',\n",
       " 'alpine',\n",
       " 'alright',\n",
       " 'altamonte',\n",
       " 'alter',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alto',\n",
       " 'alum',\n",
       " 'alumni',\n",
       " 'alumnus',\n",
       " 'alvin',\n",
       " 'alyssa',\n",
       " 'alzheimer',\n",
       " 'amalie',\n",
       " 'amanda',\n",
       " 'amarillo',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazinf',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambition',\n",
       " 'ambrose',\n",
       " 'ambulance',\n",
       " 'amc',\n",
       " 'ame',\n",
       " 'amelia',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americana',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'ameritrade',\n",
       " 'amg',\n",
       " 'amherst',\n",
       " 'amid',\n",
       " 'amigo',\n",
       " 'amp',\n",
       " 'amp226amp128amp147',\n",
       " 'ampamp',\n",
       " 'amphitheater',\n",
       " 'amphitheatre',\n",
       " 'amsterdam',\n",
       " 'amtrak',\n",
       " 'amuse',\n",
       " 'amusement',\n",
       " 'amy',\n",
       " 'ana',\n",
       " 'anaheim',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'anatomy',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'anderson',\n",
       " 'andor',\n",
       " 'andover',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andreas',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'android',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelo',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angie',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'anita',\n",
       " 'ankeny',\n",
       " 'ankle',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annapolis',\n",
       " 'anne',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announcement',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'answer',\n",
       " 'ant',\n",
       " 'antelope',\n",
       " 'anthem',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipation',\n",
       " 'antioch',\n",
       " 'antique',\n",
       " 'antonio',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'anza',\n",
       " 'ap',\n",
       " 'apache',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'apex',\n",
       " 'apocalypse',\n",
       " 'apollo',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apopka',\n",
       " 'app',\n",
       " 'apparel',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appetite',\n",
       " 'appetizer',\n",
       " 'apple',\n",
       " 'applebee',\n",
       " 'appleton',\n",
       " 'appliance',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciation',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approximately',\n",
       " 'appt',\n",
       " 'apricot',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'aqua',\n",
       " 'aquarium',\n",
       " 'aquatic',\n",
       " 'ar',\n",
       " 'arbor',\n",
       " 'arboretum',\n",
       " 'arc',\n",
       " 'arcade',\n",
       " 'arcadia',\n",
       " 'arch',\n",
       " 'architect',\n",
       " 'architecture',\n",
       " 'archive',\n",
       " 'arclight',\n",
       " 'ardsley',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'ari',\n",
       " 'aria',\n",
       " 'ariana',\n",
       " 'ariel',\n",
       " 'arizona',\n",
       " 'ark',\n",
       " 'arkansas',\n",
       " 'arkham',\n",
       " 'arlington',\n",
       " 'arm',\n",
       " 'armed',\n",
       " 'armory',\n",
       " 'arms',\n",
       " 'armstrong',\n",
       " 'army',\n",
       " 'arnold',\n",
       " 'aroma',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrogant',\n",
       " 'arrow',\n",
       " 'arrowhead',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'artichoke',\n",
       " 'article',\n",
       " 'artisan',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artists',\n",
       " 'arts',\n",
       " 'artsy',\n",
       " 'artwork',\n",
       " 'arugula',\n",
       " 'asap',\n",
       " 'asbury',\n",
       " 'asf',\n",
       " 'ash',\n",
       " 'ashamed',\n",
       " 'ashburn',\n",
       " 'asheville',\n",
       " 'ashland',\n",
       " 'ashley',\n",
       " 'ashton',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asl',\n",
       " 'asleep',\n",
       " 'asparagus',\n",
       " 'aspect',\n",
       " 'aspen',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assault',\n",
       " 'assembly',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assignment',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'associates',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'astoria',\n",
       " 'astro',\n",
       " 'astros',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model with tf-idf features,n_samples=5000000 and n_features=10000...\n",
      "done in 1125.197s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #1\n",
      "['good', 'friend', 'morning', 'life', 'feel', 'look', 'thing', 'luck', 'world', 'way', 'people', 'today', 'hope', 'bad', 'year', 'pretty', 'ask', 'think', 'god', 'weekend']\n",
      "Topic #2\n",
      "['love', 'girl', 'life', 'check', 'easy', 'darkness', 'leave', 'baby', 'guy', 'people', 'friend', 'miss', 'ya', 'man', 'little', 'place', 'fall', 'hate', 'vid', 'beautiful']\n",
      "Topic #3\n",
      "['just', 'post', 'photo', 'video', 'park', 'add', 'menu', 'beer', 'lake', 'center', 'tap', 'museum', 'world', 'city', 'island', 'state', 'house', 'bridge', 'point', 'national']\n",
      "Topic #4\n",
      "['day', 'father', 'great', 'beautiful', 'today', 'dad', 'school', 'national', 'start', 'long', 'spend', 'summer', 'fathers', 'happy', 'lake', 'hope', 'perfect', 'fun', 'enjoy', 'tomorrow']\n",
      "Topic #5\n",
      "['new', 'york', 'ny', 'city', 'brooklyn', 'event', 'check', 'square', 'orleans', 'park', 'jersey', 'nyc', 'times', 'world', 'center', 'museum', 'airport', 'manhattan', 'friend', 'international']\n",
      "Topic #6\n",
      "['thank', 'follow', 'god', 'great', 'check', 'amazing', 'darkness', 'support', 'easy', 'leave', 'man', 'awesome', 'miss', 'friend', 'bro', 'make', 'guy', 'year', 'passion', 'hidden']\n",
      "Topic #7\n",
      "['today', 'mph', 'rain', 'wind', 'humidity', 'temperature', 'barometer', '000', '00', 'gt', 'steady', 'fall', 'rise', 'slowly', 'pressure', 'temp', '10', 'cloudy', 'gust', 'mb']\n",
      "Topic #8\n",
      "['like', 'look', 'feel', 'shit', 'think', 'people', 'fuck', 'girl', 'say', 'bitch', 'buyer', 'act', 'sound', 'talk', 'right', 'man', 'thing', 'hate', 'year', 'make']\n",
      "Topic #9\n",
      "['happy', 'birthday', 'father', 'hope', 'friend', 'girl', 'dad', 'make', 'man', 'year', 'bday', 'wish', 'friday', 'hour', 'miss', 'place', 'celebrate', 'fathers', 'big', 'amazing']\n",
      "Topic #10\n",
      "['time', 'great', 'long', 'year', 'think', 'game', 'fun', 'wait', 'life', 'spend', 'summer', 'watch', 'week', 'people', 'waste', 'family', 'play', 'little', 'start', 'run']\n",
      "Topic #11\n",
      "['know', 'let', 'think', 'people', 'right', 'shit', 'thing', 'girl', 'mean', 'wanna', 'talk', 'tell', 'ya', 'man', 'fuck', 'say', 'guy', 'real', 'oh', 'happen']\n",
      "Topic #12\n",
      "['amp', 'bar', 'grill', 'st', 'tonight', 'restaurant', 'open', 'street', 'intersection', 'request', 'case', 'friend', 'ca', 'lounge', 'house', 'hotel', 'close', 'graffiti', 'girl', 'closed']\n",
      "Topic #13\n",
      "['lol', 'think', 'shit', 'fuck', 'right', 'oh', 'yeah', 'say', 'yes', 'tho', 'tell', 'try', 'funny', 'man', 'nigga', 'talk', 'bad', 'girl', 'ass', 'damn']\n",
      "Topic #14\n",
      "['come', 'home', 'tonight', 'soon', 'wait', 'baby', 'way', 'today', 'check', 'tomorrow', 'mango', 'party', 'visit', 'play', 'ready', 'true', 'watch', 'week', 'let', 'game']\n",
      "Topic #15\n",
      "['portland', 'block', 'check', 'police', 'st', 'ave', 'se', 'ne', 'medical', 'med', 'priority', 'sw', 'cold', 'gresham', 'person', 'nw', 'disturbance', 'blvd', 'accident', 'welfare']\n",
      "Topic #16\n",
      "['drink', 'ipa', 'ale', 'beer', 'brewing', 'pale', 'house', 'summer', 'company', 'stout', '2015', 'hop', 'brewery', 'bar', 'lager', 'red', 'double', 'white', 'wheat', 'pub']\n",
      "Topic #17\n",
      "['want', 'bad', 'life', 'fuck', 'people', 'right', 'think', 'talk', 'thing', 'baby', 'tell', 'leave', 'girl', 'sleep', 'say', 'play', 'man', 'watch', 'home', 'shit']\n",
      "Topic #18\n",
      "['night', 'great', 'tonight', 'fun', 'late', 'friday', 'sleep', 'summer', 'tomorrow', 'girl', 'saturday', 'date', 'beautiful', 'game', 'amazing', 'party', 'club', 'watch', 'live', 'park']\n",
      "Topic #19\n",
      "['work', 'need', 'hard', 'life', 'today', 'sleep', 'right', 'think', 'home', 'fuck', 'week', 'people', 'way', 'tomorrow', 'let', 'start', 'hour', 'stop', 'thing', 'ready']\n",
      "Topic #20\n",
      "['beach', 'miss', 'park', 'south', 'miami', 'fl', 'ca', 'city', 'life', 'long', 'island', 'state', 'san', 'florida', 'ocean', 'lake', 'north', 'myrtle', 'venice', 'club']\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features,\"\n",
    "      \"n_samples=%d and n_features=%d...\" % (len(text_tokenized[:5000000]), num_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "exit()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_topics(nmf, tfidf_feature_names, top_k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
